{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API Base URL: http://localhost:8080/api\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import time\n",
    "import asyncio\n",
    "import aiohttp\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import nest_asyncio\n",
    "import json\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "API_BASE_URL = \"http://localhost:8080/api\"\n",
    "\n",
    "print(f\"API Base URL: {API_BASE_URL}\\n\")\n",
    "\n",
    "# --- Helper function to make requests ---\n",
    "def make_request(method, endpoint, payload=None, expected_status=200):\n",
    "    url = f\"{API_BASE_URL}{endpoint}\"\n",
    "    try:\n",
    "        if method.upper() == \"GET\":\n",
    "            response = requests.get(url, timeout=60)\n",
    "        elif method.upper() == \"POST\":\n",
    "            response = requests.post(url, json=payload, timeout=120) # Longer timeout for predict\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported method: {method}\")\n",
    "        \n",
    "        print(f\"--- Request to {method.upper()} {endpoint} ---\")\n",
    "        if payload:\n",
    "            print(f\"Payload (first 50 chars if long): {str(payload)[:150]}\")\n",
    "        \n",
    "        if response.status_code == expected_status:\n",
    "            print(f\"Status: {response.status_code} OK\")\n",
    "            try:\n",
    "                res_json = response.json()\n",
    "                print(f\"Response (sample): {str(res_json)[:300]}...\")\n",
    "                return res_json\n",
    "            except requests.exceptions.JSONDecodeError:\n",
    "                print(f\"Response (not JSON): {response.text[:300]}...\")\n",
    "                return response.text\n",
    "        else:\n",
    "            print(f\"Status: {response.status_code} - Error: {response.text[:300]}...\")\n",
    "            return {\"error\": response.text, \"status_code\": response.status_code}\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Request failed for {endpoint}: {e}\")\n",
    "        return {\"error\": str(e), \"status_code\": \"N/A\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Checking Basic API Endpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Checking Basic API Endpoints ---\n",
      "--- Request to GET / ---\n",
      "Status: 200 OK\n",
      "Response (sample): {'message': 'HuggingFace Dynamic Inference API', 'active_device': 'GPU', 'default_model_status': 'Preload attempted, cache populated', 'current_cache_size': 2}...\n",
      "--- Request to GET /healthz ---\n",
      "Status: 200 OK\n",
      "Response (sample): {'status': 'ok', 'pid': 26}...\n",
      "--- Request to GET /readiness ---\n",
      "Status: 200 OK\n",
      "Response (sample): {'status': 'ready', 'message': \"Default model 'distilbert-base-uncased-finetuned-sst-2-english' accessible/loadable.\", 'pid': 26}...\n",
      "--- Request to GET /metrics ---\n",
      "Status: 200 OK\n",
      "Response (not JSON): # HELP hf_requests_total Total number of inference requests\n",
      "# TYPE hf_requests_total counter\n",
      "hf_requests_total{http_status=\"200\",model_name=\"gpt2\",task=\"text-generation\",worker_pid=\"25\"} 2.0\n",
      "hf_requests_total{http_status=\"200\",model_name=\"distilbert-base-uncased-finetuned-sst-2-english\",task=\"sentim...\n",
      "--- Request to GET /cache_info ---\n",
      "Status: 200 OK\n",
      "Response (sample): {'pid': 26, 'lru_cache_stats': 'CacheInfo(hits=9, misses=2, maxsize=10, currsize=2)'}...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'pid': 26,\n",
       " 'lru_cache_stats': 'CacheInfo(hits=9, misses=2, maxsize=10, currsize=2)'}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"\\n--- Checking Basic API Endpoints ---\")\n",
    "make_request(\"GET\", \"/\")\n",
    "make_request(\"GET\", \"/healthz\")\n",
    "make_request(\"GET\", \"/readiness\") # This might fail if default model preload fails\n",
    "make_request(\"GET\", \"/metrics\") # Check Prometheus metrics endpoint\n",
    "make_request(\"GET\", \"/cache_info\") # Check cache on one worker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Single Sentiment Prediction Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Test Single Prediction (Default Model) ---\n",
      "--- Request to POST /predict ---\n",
      "Payload (first 50 chars if long): {'model_name': 'distilbert-base-uncased-finetuned-sst-2-english', 'task': 'sentiment-analysis', 'inputs': 'This is a fantastic product, I highly recom\n",
      "Status: 200 OK\n",
      "Response (sample): {'model_name': 'distilbert-base-uncased-finetuned-sst-2-english', 'task': 'sentiment-analysis', 'predictions': [{'label': 'POSITIVE', 'score': 0.9998854398727417}], 'worker_pid': 27, 'total_request_time_ms': 7.02, 'inference_execution_time_ms': 6.88, 'pipeline_from_lru_cache': True}...\n",
      "\n",
      "Response:\n",
      "model_name: distilbert-base-uncased-finetuned-sst-2-english\n",
      "task: sentiment-analysis\n",
      "predictions: [{'label': 'POSITIVE', 'score': 0.9998854398727417}]\n",
      "worker_pid: 27\n",
      "total_request_time_ms: 7.02\n",
      "inference_execution_time_ms: 6.88\n",
      "pipeline_from_lru_cache: True\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- Test Single Prediction (Default Model) ---\")\n",
    "single_payload_sentiment = {\n",
    "    \"model_name\": \"distilbert-base-uncased-finetuned-sst-2-english\",\n",
    "    \"task\": \"sentiment-analysis\",\n",
    "    \"inputs\": \"This is a fantastic product, I highly recommend it!\"\n",
    "}\n",
    "result_single = make_request(\"POST\", \"/predict\", single_payload_sentiment)\n",
    "print(\"\\nResponse:\")\n",
    "for key, val in result_single.items():\n",
    "    print(f\"{key}: {val}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Batch Sentiment Prediction Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Test Batch Prediction (Default Model) ---\n",
      "--- Request to POST /predict ---\n",
      "Payload (first 50 chars if long): {'model_name': 'distilbert-base-uncased-finetuned-sst-2-english', 'task': 'sentiment-analysis', 'inputs': ['I am incredibly happy with the service.', \n",
      "Status: 200 OK\n",
      "Response (sample): {'model_name': 'distilbert-base-uncased-finetuned-sst-2-english', 'task': 'sentiment-analysis', 'predictions': [{'label': 'POSITIVE', 'score': 0.9998799562454224}, {'label': 'NEGATIVE', 'score': 0.9997836947441101}, {'label': 'POSITIVE', 'score': 0.9969683289527893}], 'worker_pid': 27, 'total_reques...\n",
      "\n",
      "Response:\n",
      "model_name: distilbert-base-uncased-finetuned-sst-2-english\n",
      "task: sentiment-analysis\n",
      "predictions: [{'label': 'POSITIVE', 'score': 0.9998799562454224}, {'label': 'NEGATIVE', 'score': 0.9997836947441101}, {'label': 'POSITIVE', 'score': 0.9969683289527893}]\n",
      "worker_pid: 27\n",
      "total_request_time_ms: 20.76\n",
      "inference_execution_time_ms: 20.63\n",
      "pipeline_from_lru_cache: True\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- Test Batch Prediction (Default Model) ---\")\n",
    "batch_payload_sentiment = {\n",
    "    \"model_name\": \"distilbert-base-uncased-finetuned-sst-2-english\",\n",
    "    \"task\": \"sentiment-analysis\",\n",
    "    \"inputs\": [\n",
    "        \"I am incredibly happy with the service.\",\n",
    "        \"This is the worst thing I have ever bought.\",\n",
    "        \"It's an okay movie, neither good nor bad.\"\n",
    "    ]\n",
    "}\n",
    "result_batch = make_request(\"POST\", \"/predict\", batch_payload_sentiment)\n",
    "print(\"\\nResponse:\")\n",
    "for key, val in result_batch.items():\n",
    "    print(f\"{key}: {val}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Test a Different Model and Task (Text Generation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Test Different Model/Task (Text Generation with GPT-2) ---\n",
      "--- Request to POST /predict ---\n",
      "Payload (first 50 chars if long): {'model_name': 'gpt2', 'task': 'text-generation', 'inputs': 'Once upon a time, in a land far away', 'pipeline_kwargs': {'max_new_tokens': 20, 'num_ret\n",
      "Status: 200 OK\n",
      "Response (sample): {'model_name': 'gpt2', 'task': 'text-generation', 'predictions': [{'generated_text': 'Once upon a time, in a land far away, a man, the son of a rich merchant of a wealthy family, was caught in the midst'}], 'worker_pid': 28, 'total_request_time_ms': 153.7, 'inference_execution_time_ms': 153.56, 'pip...\n",
      "\n",
      "Response:\n",
      "model_name: gpt2\n",
      "task: text-generation\n",
      "predictions: [{'generated_text': 'Once upon a time, in a land far away, a man, the son of a rich merchant of a wealthy family, was caught in the midst'}]\n",
      "worker_pid: 28\n",
      "total_request_time_ms: 153.7\n",
      "inference_execution_time_ms: 153.56\n",
      "pipeline_from_lru_cache: True\n"
     ]
    }
   ],
   "source": [
    "# This will cause a new model to be downloaded and cached if not used before.\n",
    "print(\"\\n--- Test Different Model/Task (Text Generation with GPT-2) ---\")\n",
    "# For actual generation, you might want 'gpt2-medium' or larger.\n",
    "generation_payload = {\n",
    "    \"model_name\": \"gpt2\",\n",
    "    \"task\": \"text-generation\",\n",
    "    \"inputs\": \"Once upon a time, in a land far away\",\n",
    "    \"pipeline_kwargs\": {\"max_new_tokens\": 20, \"num_return_sequences\": 1} # Arguments for the pipeline\n",
    "}\n",
    "# Text generation can take longer, especially for the first load.\n",
    "result_generation = make_request(\"POST\", \"/predict\", generation_payload)\n",
    "\n",
    "print(\"\\nResponse:\")\n",
    "for key, val in result_generation.items():\n",
    "    print(f\"{key}: {val}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parallel Requests using `ThreadPoolExecutor`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Demonstrate Parallel Requests (ThreadPoolExecutor) ---\n",
      "Parallel Req 1/16:\tPID: 27 | Predictions:'[{'label': 'POSITIVE', 'score': 0.9998791217803955}]' | Total Request Time (ms): '19.96'\n",
      "Parallel Req 2/16:\tPID: 26 | Predictions:'[{'label': 'NEGATIVE', 'score': 0.9992559552192688}]' | Total Request Time (ms): '21.83'\n",
      "Parallel Req 3/16:\tPID: 25 | Predictions:'[{'label': 'NEGATIVE', 'score': 0.9997283816337585}]' | Total Request Time (ms): '23.3'\n",
      "Parallel Req 4/16:\tPID: 28 | Predictions:'[{'label': 'POSITIVE', 'score': 0.9998494386672974}]' | Total Request Time (ms): '21.97'\n",
      "Parallel Req 5/16:\tPID: 27 | Predictions:'[{'label': 'POSITIVE', 'score': 0.9997918009757996}]' | Total Request Time (ms): '8.01'\n",
      "Parallel Req 6/16:\tPID: 25 | Predictions:'[{'label': 'NEGATIVE', 'score': 0.9997954964637756}]' | Total Request Time (ms): '6.49'\n",
      "Parallel Req 7/16:\tPID: 26 | Predictions:'[{'label': 'POSITIVE', 'score': 0.9998459815979004}]' | Total Request Time (ms): '7.31'\n",
      "Parallel Req 8/16:\tPID: 28 | Predictions:'[{'label': 'POSITIVE', 'score': 0.9998494386672974}]' | Total Request Time (ms): '6.19'\n",
      "Parallel Req 9/16:\tPID: 27 | Predictions:'[{'label': 'NEGATIVE', 'score': 0.9865264296531677}]' | Total Request Time (ms): '8.25'\n",
      "Parallel Req 10/16:\tPID: 26 | Predictions:'[{'label': 'NEGATIVE', 'score': 0.9997283816337585}]' | Total Request Time (ms): '5.41'\n",
      "Parallel Req 11/16:\tPID: 25 | Predictions:'[{'label': 'POSITIVE', 'score': 0.9998791217803955}]' | Total Request Time (ms): '6.7'\n",
      "Parallel Req 12/16:\tPID: 28 | Predictions:'[{'label': 'NEGATIVE', 'score': 0.9865264296531677}]' | Total Request Time (ms): '6.94'\n",
      "Parallel Req 13/16:\tPID: 26 | Predictions:'[{'label': 'POSITIVE', 'score': 0.9997918009757996}]' | Total Request Time (ms): '5.23'\n",
      "Parallel Req 14/16:\tPID: 27 | Predictions:'[{'label': 'NEGATIVE', 'score': 0.9992559552192688}]' | Total Request Time (ms): '6.58'\n",
      "Parallel Req 15/16:\tPID: 25 | Predictions:'[{'label': 'NEGATIVE', 'score': 0.9997954964637756}]' | Total Request Time (ms): '6.34'\n",
      "Parallel Req 16/16:\tPID: 28 | Predictions:'[{'label': 'POSITIVE', 'score': 0.9998459815979004}]' | Total Request Time (ms): '7.62'\n",
      "\n",
      "ThreadPoolExecutor: Completed 16 sentiment requests in 0.06 seconds.\n",
      "Sentiment requests handled by PIDs: {25, 26, 27, 28} (16 successful)\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- Demonstrate Parallel Requests (ThreadPoolExecutor) ---\")\n",
    "parallel_texts_sentiment = [\n",
    "    \"The weather today is beautiful and sunny.\",\n",
    "    \"I'm feeling a bit down after hearing the news.\",\n",
    "    \"This new software update is incredibly buggy.\",\n",
    "    \"The concert was an unforgettable experience!\",\n",
    "    \"Customer support was surprisingly helpful and efficient.\",\n",
    "    \"I am neutral about this new policy change.\",\n",
    "    \"This book is a masterpiece of modern literature.\",\n",
    "    \"The food at that restaurant was utterly disappointing.\"\n",
    "] * 2 # 16 requests\n",
    "\n",
    "def send_predict_request(text_input, model_name, task):\n",
    "    payload = {\"model_name\": model_name, \"task\": task, \"inputs\": text_input}\n",
    "    url = f\"{API_BASE_URL}/predict\"\n",
    "    try:\n",
    "        response = requests.post(url, json=payload, timeout=120)\n",
    "        response.raise_for_status()\n",
    "        return response.json()\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        return {\"error\": str(e), \"input_text\": text_input, \"status_code\": response.status_code if 'response' in locals() else \"N/A\"}\n",
    "\n",
    "start_time_parallel = time.time()\n",
    "parallel_results_sentiment = []\n",
    "# Aggressively use workers to demonstrate server parallelism\n",
    "with ThreadPoolExecutor(max_workers=len(parallel_texts_sentiment)) as executor:\n",
    "    futures = [\n",
    "        executor.submit(send_predict_request, text, \"distilbert-base-uncased-finetuned-sst-2-english\", \"sentiment-analysis\")\n",
    "        for text in parallel_texts_sentiment\n",
    "    ]\n",
    "    for i, future in enumerate(as_completed(futures)):\n",
    "        try:\n",
    "            data = future.result()\n",
    "            parallel_results_sentiment.append(data)\n",
    "            print(f\"Parallel Req {i+1}/{len(parallel_texts_sentiment)}:\\tPID: {data.get('worker_pid', 'N/A')} | Predictions:'{str(data.get('predictions', 'N/A'))}' | Total Request Time (ms): '{str(data.get('total_request_time_ms', 'N/A'))}'\")\n",
    "        except Exception as exc:\n",
    "            print(f\"Parallel Req {i+1}/{len(parallel_texts_sentiment)} generated an exception: {exc}\")\n",
    "            parallel_results_sentiment.append({\"error\": str(exc)})\n",
    "\n",
    "end_time_parallel = time.time()\n",
    "print(f\"\\nThreadPoolExecutor: Completed {len(parallel_results_sentiment)} sentiment requests in {end_time_parallel - start_time_parallel:.2f} seconds.\")\n",
    "\n",
    "worker_pids_sentiment = set()\n",
    "successful_sentiment_requests = 0\n",
    "for res in parallel_results_sentiment:\n",
    "    if isinstance(res, dict) and \"worker_pid\" in res:\n",
    "        worker_pids_sentiment.add(res['worker_pid'])\n",
    "        successful_sentiment_requests +=1\n",
    "print(f\"Sentiment requests handled by PIDs: {worker_pids_sentiment} ({successful_sentiment_requests} successful)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Demonstrate Parallel Text Generation (ThreadPoolExecutor) ---\n",
      "Sending 16 text generation requests with 4 parallel client threads...\n",
      "Parallel Gen Req 1/16:\tPID: 28 | Output: 'The secret to a happy life is the desire to make it in the world. And that's what we do here in Cana'\t| Time(ms): 1137.43\n",
      "Parallel Gen Req 2/16:\tPID: 27 | Output: 'Once upon a time, in a land far away, were not the birds of the sky. The world was never in a sense '\t| Time(ms): 1138.69\n",
      "Parallel Gen Req 3/16:\tPID: 26 | Output: 'My favorite a_model_prompt is always \"You're not playing at this time.\"  If you're getting a call fr'\t| Time(ms): 1156.39\n",
      "Parallel Gen Req 4/16:\tPID: 25 | Output: 'Artificial intelligence will eventually be the next big thing.  At the same time, AI will only be ab'\t| Time(ms): 1158.08\n",
      "Parallel Gen Req 5/16:\tPID: 27 | Output: 'The future of space exploration looks bright, but it's not ready yet.\"  NASA has been working with t'\t| Time(ms): 1111.38\n",
      "Parallel Gen Req 6/16:\tPID: 28 | Output: 'To build a successful startup, one must work hard to be successful. We've got to build a business.  '\t| Time(ms): 1113.1\n",
      "Parallel Gen Req 7/16:\tPID: 26 | Output: 'If I could travel anywhere in time, I would go to the beach, but there is something about the way th'\t| Time(ms): 1111.18\n",
      "Parallel Gen Req 8/16:\tPID: 25 | Output: 'A recipe for a perfect day includes a sweetened condensed milk. The milk is a blend of milk and suga'\t| Time(ms): 1113.05\n",
      "Parallel Gen Req 9/16:\tPID: 27 | Output: 'The secret to a happy life is not the money, but the people.  But as the government turns its back o'\t| Time(ms): 1115.98\n",
      "Parallel Gen Req 10/16:\tPID: 28 | Output: 'Once upon a time, in a land far away, a human being would be able to do that.  In a world that lacks'\t| Time(ms): 1122.76\n",
      "Parallel Gen Req 11/16:\tPID: 25 | Output: 'My favorite a_model_prompt is the \"A_model_prompt\" class, and it works best with a viewmodel.  A_mod'\t| Time(ms): 1117.8\n",
      "Parallel Gen Req 12/16:\tPID: 26 | Output: 'Artificial intelligence will eventually be able to better manage the daily routines of human beings.'\t| Time(ms): 1127.59\n",
      "Parallel Gen Req 13/16:\tPID: 27 | Output: 'To build a successful startup, one must first design and implement a business plan to meet the parti'\t| Time(ms): 1114.31\n",
      "Parallel Gen Req 14/16:\tPID: 28 | Output: 'The future of space exploration looks like a different story.  Space shuttle Launch Complex 1 is now'\t| Time(ms): 1118.39\n",
      "Parallel Gen Req 15/16:\tPID: 25 | Output: 'A recipe for a perfect day includes:  1. The top of each piece of the cake and the top of the lid.  '\t| Time(ms): 1117.43\n",
      "Parallel Gen Req 16/16:\tPID: 26 | Output: 'If I could travel anywhere in time, I would go to the airport, then to the train station. I would go'\t| Time(ms): 1117.57\n",
      "\n",
      "ThreadPoolExecutor: Completed 16 text generation requests in 4.54 seconds.\n",
      "Text generation requests handled successfully by PIDs: {25, 26, 27, 28} (16 successful out of 16 attempts)\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- Demonstrate Parallel Text Generation (ThreadPoolExecutor) ---\")\n",
    "\n",
    "# Prompts for text generation\n",
    "parallel_prompts_generation = [\n",
    "    \"Once upon a time, in a land far away,\",\n",
    "    \"The secret to a happy life is\",\n",
    "    \"Artificial intelligence will eventually\",\n",
    "    \"My favorite a_model_prompt is\",\n",
    "    \"To build a successful startup, one must\",\n",
    "    \"The future of space exploration looks\",\n",
    "    \"A recipe for a perfect day includes\",\n",
    "    \"If I could travel anywhere in time, I would go to\"\n",
    "]*2 \n",
    "\n",
    "# Updated function to include pipeline_kwargs\n",
    "def send_predict_request_with_kwargs(prompt_input, model_name, task, pipeline_kwargs=None):\n",
    "    if pipeline_kwargs is None:\n",
    "        pipeline_kwargs = {}\n",
    "        \n",
    "    payload = {\n",
    "        \"model_name\": model_name,\n",
    "        \"task\": task,\n",
    "        \"inputs\": prompt_input,\n",
    "        \"pipeline_kwargs\": pipeline_kwargs\n",
    "    }\n",
    "    url = f\"{API_BASE_URL}/predict\" \n",
    "    try:\n",
    "        # Increased timeout for text generation\n",
    "        response = requests.post(url, json=payload, timeout=180) \n",
    "        response.raise_for_status()\n",
    "        return response.json()\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        # Try to get status code from response if available\n",
    "        status_code = \"N/A\"\n",
    "        if hasattr(e, 'response') and e.response is not None:\n",
    "            status_code = e.response.status_code\n",
    "        return {\"error\": str(e), \"input_text\": prompt_input, \"status_code\": status_code}\n",
    "    except Exception as ex: # Catch other potential errors during request construction or handling\n",
    "        return {\"error\": f\"Unexpected error: {str(ex)}\", \"input_text\": prompt_input, \"status_code\": \"N/A\"}\n",
    "\n",
    "\n",
    "start_time_parallel_gen = time.time()\n",
    "parallel_results_generation = []\n",
    "\n",
    "# Define pipeline_kwargs for text generation\n",
    "generation_kwargs = {\n",
    "    \"max_new_tokens\": 100,      # Generate a small number of new tokens for the demo\n",
    "    \"num_return_sequences\": 1, # Get one generated sequence per prompt\n",
    "}\n",
    "\n",
    "num_client_threads = min(4, len(parallel_prompts_generation)) \n",
    "print(f\"Sending {len(parallel_prompts_generation)} text generation requests with {num_client_threads} parallel client threads...\")\n",
    "\n",
    "with ThreadPoolExecutor(max_workers=num_client_threads) as executor:\n",
    "    futures = [\n",
    "        executor.submit(send_predict_request_with_kwargs, prompt, \"gpt2\", \"text-generation\", generation_kwargs)\n",
    "        for prompt in parallel_prompts_generation\n",
    "    ]\n",
    "    for i, future in enumerate(as_completed(futures)):\n",
    "        try:\n",
    "            data = future.result()\n",
    "            # It's good to check if 'data' is an error dict before trying to access specific keys\n",
    "            if \"error\" in data:\n",
    "                print(f\"Parallel Gen Req {i+1}/{len(parallel_prompts_generation)} -> ERROR: {data.get('error')} (Status: {data.get('status_code', 'N/A')}) | Input: '{data.get('input_text', 'N/A')[:50]}...'\")\n",
    "            else:\n",
    "                generated_texts = data.get('predictions', [])\n",
    "                display_text = \"N/A\"\n",
    "                if generated_texts and isinstance(generated_texts, list) and len(generated_texts) > 0:\n",
    "                    # Display the first generated text for brevity\n",
    "                    if isinstance(generated_texts[0], dict) and \"generated_text\" in generated_texts[0]:\n",
    "                        display_text = generated_texts[0][\"generated_text\"].replace('\\n', ' ')[:100] # First 100 chars, newlines replaced\n",
    "                    else: # If structure is different, just show raw\n",
    "                        display_text = str(generated_texts[0])\n",
    "\n",
    "\n",
    "                print(f\"Parallel Gen Req {i+1}/{len(parallel_prompts_generation)}:\\tPID: {data.get('worker_pid', 'N/A')} | Output: '{display_text}'\\t| Time(ms): {data.get('total_request_time_ms', 'N/A')}\")\n",
    "        except Exception as exc: # Should ideally be caught by error handling in send_predict_request_with_kwargs\n",
    "            print(f\"Parallel Gen Req {i+1}/{len(parallel_prompts_generation)} generated an exception in future processing: {exc}\")\n",
    "            parallel_results_generation.append({\"error\": str(exc)})\n",
    "        else: # Append result only if no exception during future.result() or processing\n",
    "             parallel_results_generation.append(data)\n",
    "\n",
    "\n",
    "end_time_parallel_gen = time.time()\n",
    "print(f\"\\nThreadPoolExecutor: Completed {len(parallel_results_generation)} text generation requests in {end_time_parallel_gen - start_time_parallel_gen:.2f} seconds.\")\n",
    "\n",
    "worker_pids_generation = set()\n",
    "successful_generation_requests = 0\n",
    "for res in parallel_results_generation:\n",
    "    if isinstance(res, dict) and \"worker_pid\" in res and \"error\" not in res: # Count successful requests\n",
    "        worker_pids_generation.add(res['worker_pid'])\n",
    "        successful_generation_requests +=1\n",
    "print(f\"Text generation requests handled successfully by PIDs: {worker_pids_generation} ({successful_generation_requests} successful out of {len(parallel_prompts_generation)} attempts)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
