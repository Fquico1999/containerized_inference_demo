{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import time\n",
    "import asyncio\n",
    "import aiohttp\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import nest_asyncio\n",
    "import json\n",
    "\n",
    "nest_asyncio.apply() # For Jupyter\n",
    "\n",
    "API_BASE_URL = \"http://localhost:8000\" # Ensure container is running and port-mapped\n",
    "\n",
    "print(f\"API Base URL: {API_BASE_URL}\\n\")\n",
    "\n",
    "# --- Helper function to make requests ---\n",
    "def make_request(method, endpoint, payload=None, expected_status=200):\n",
    "    url = f\"{API_BASE_URL}{endpoint}\"\n",
    "    try:\n",
    "        if method.upper() == \"GET\":\n",
    "            response = requests.get(url, timeout=60)\n",
    "        elif method.upper() == \"POST\":\n",
    "            response = requests.post(url, json=payload, timeout=120) # Longer timeout for predict\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported method: {method}\")\n",
    "        \n",
    "        print(f\"--- Request to {method.upper()} {endpoint} ---\")\n",
    "        if payload:\n",
    "            print(f\"Payload (first 50 chars if long): {str(payload)[:150]}\")\n",
    "        \n",
    "        if response.status_code == expected_status:\n",
    "            print(f\"Status: {response.status_code} OK\")\n",
    "            try:\n",
    "                res_json = response.json()\n",
    "                print(f\"Response (sample): {str(res_json)[:300]}...\")\n",
    "                return res_json\n",
    "            except requests.exceptions.JSONDecodeError:\n",
    "                print(f\"Response (not JSON): {response.text[:300]}...\")\n",
    "                return response.text\n",
    "        else:\n",
    "            print(f\"Status: {response.status_code} - Error: {response.text[:300]}...\")\n",
    "            return {\"error\": response.text, \"status_code\": response.status_code}\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Request failed for {endpoint}: {e}\")\n",
    "        return {\"error\": str(e), \"status_code\": \"N/A\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. Check Basic API Endpoints ---\n",
    "print(\"\\n--- Checking Basic API Endpoints ---\")\n",
    "make_request(\"GET\", \"/\")\n",
    "make_request(\"GET\", \"/healthz\")\n",
    "make_request(\"GET\", \"/readiness\") # This might fail if default model preload fails\n",
    "make_request(\"GET\", \"/metrics\") # Check Prometheus metrics endpoint\n",
    "make_request(\"GET\", \"/cache_info\") # Check cache on one worker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 2. Test Single Prediction (Default Model) ---\n",
    "print(\"\\n--- Test Single Prediction (Default Model) ---\")\n",
    "single_payload_sentiment = {\n",
    "    \"model_name\": \"distilbert-base-uncased-finetuned-sst-2-english\",\n",
    "    \"task\": \"sentiment-analysis\",\n",
    "    \"inputs\": \"This is a fantastic product, I highly recommend it!\"\n",
    "}\n",
    "result_single = make_request(\"POST\", \"/predict\", single_payload_sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 3. Test Batch Prediction (Default Model) ---\n",
    "print(\"\\n--- Test Batch Prediction (Default Model) ---\")\n",
    "batch_payload_sentiment = {\n",
    "    \"model_name\": \"distilbert-base-uncased-finetuned-sst-2-english\",\n",
    "    \"task\": \"sentiment-analysis\",\n",
    "    \"inputs\": [\n",
    "        \"I am incredibly happy with the service.\",\n",
    "        \"This is the worst thing I have ever bought.\",\n",
    "        \"It's an okay movie, neither good nor bad.\"\n",
    "    ]\n",
    "}\n",
    "result_batch = make_request(\"POST\", \"/predict\", batch_payload_sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 4. Test a Different Model and Task (e.g., Text Generation) ---\n",
    "# This will cause a new model to be downloaded and cached if not used before.\n",
    "print(\"\\n--- Test Different Model/Task (Text Generation with GPT-2) ---\")\n",
    "# Using a smaller variant like 'gpt2' for quicker demo.\n",
    "# For actual generation, you might want 'gpt2-medium' or larger.\n",
    "generation_payload = {\n",
    "    \"model_name\": \"gpt2\", # A common, relatively small text generation model\n",
    "    \"task\": \"text-generation\",\n",
    "    \"inputs\": \"Once upon a time, in a land far away\",\n",
    "    \"pipeline_kwargs\": {\"max_new_tokens\": 20, \"num_return_sequences\": 1} # Arguments for the pipeline\n",
    "}\n",
    "# Text generation can take longer, especially for the first load.\n",
    "result_generation = make_request(\"POST\", \"/predict\", generation_payload)\n",
    "# Verify cache info again (might be a different worker, but lru_cache for gpt2 should be populated in at least one)\n",
    "make_request(\"GET\", \"/cache_info\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 5. Demonstrate Parallel Requests (ThreadPoolExecutor) ---\n",
    "print(\"\\n--- Demonstrate Parallel Requests (ThreadPoolExecutor) ---\")\n",
    "parallel_texts_sentiment = [\n",
    "    \"The weather today is beautiful and sunny.\",\n",
    "    \"I'm feeling a bit down after hearing the news.\",\n",
    "    \"This new software update is incredibly buggy.\",\n",
    "    \"The concert was an unforgettable experience!\",\n",
    "    \"Customer support was surprisingly helpful and efficient.\",\n",
    "    \"I am neutral about this new policy change.\",\n",
    "    \"This book is a masterpiece of modern literature.\",\n",
    "    \"The food at that restaurant was utterly disappointing.\"\n",
    "] * 2 # 16 requests\n",
    "\n",
    "def send_predict_request(text_input, model_name, task):\n",
    "    payload = {\"model_name\": model_name, \"task\": task, \"inputs\": text_input}\n",
    "    url = f\"{API_BASE_URL}/predict\"\n",
    "    try:\n",
    "        response = requests.post(url, json=payload, timeout=120)\n",
    "        response.raise_for_status()\n",
    "        return response.json()\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        return {\"error\": str(e), \"input_text\": text_input, \"status_code\": response.status_code if 'response' in locals() else \"N/A\"}\n",
    "\n",
    "start_time_parallel = time.time()\n",
    "parallel_results_sentiment = []\n",
    "# Aggressively use workers to demonstrate server parallelism\n",
    "with ThreadPoolExecutor(max_workers=len(parallel_texts_sentiment)) as executor:\n",
    "    futures = [\n",
    "        executor.submit(send_predict_request, text, \"distilbert-base-uncased-finetuned-sst-2-english\", \"sentiment-analysis\")\n",
    "        for text in parallel_texts_sentiment\n",
    "    ]\n",
    "    for i, future in enumerate(as_completed(futures)):\n",
    "        try:\n",
    "            data = future.result()\n",
    "            parallel_results_sentiment.append(data)\n",
    "            print(f\"Parallel Req {i+1}/{len(parallel_texts_sentiment)}: PID {data.get('worker_pid', 'N/A')}, Model '{data.get('model_name', 'N/A')}', Input '{str(data.get('inputs', 'N/A'))[:30]}...'\")\n",
    "        except Exception as exc:\n",
    "            print(f\"Parallel Req {i+1}/{len(parallel_texts_sentiment)} generated an exception: {exc}\")\n",
    "            parallel_results_sentiment.append({\"error\": str(exc)})\n",
    "\n",
    "end_time_parallel = time.time()\n",
    "print(f\"\\nThreadPoolExecutor: Completed {len(parallel_results_sentiment)} sentiment requests in {end_time_parallel - start_time_parallel:.2f} seconds.\")\n",
    "\n",
    "worker_pids_sentiment = set()\n",
    "successful_sentiment_requests = 0\n",
    "for res in parallel_results_sentiment:\n",
    "    if isinstance(res, dict) and \"worker_pid\" in res:\n",
    "        worker_pids_sentiment.add(res['worker_pid'])\n",
    "        successful_sentiment_requests +=1\n",
    "print(f\"Sentiment requests handled by PIDs: {worker_pids_sentiment} ({successful_sentiment_requests} successful)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 6. Check Metrics Again After Load ---\n",
    "print(\"\\n--- Final check of /metrics endpoint ---\")\n",
    "# Metrics should reflect the requests made\n",
    "metrics_after_load = make_request(\"GET\", \"/metrics\")\n",
    "if isinstance(metrics_after_load, str): # if it's raw text from prometheus\n",
    "    # Look for specific metrics\n",
    "    if \"hf_requests_total\" in metrics_after_load and \"gpt2\" in metrics_after_load:\n",
    "        print(\"Metrics endpoint seems to contain data for requests made.\")\n",
    "    else:\n",
    "        print(\"Metrics endpoint content doesn't explicitly show expected counters, but was fetched.\")\n",
    "\n",
    "print(\"\\n--- Demo Complete ---\")\n",
    "print(\"Remember to check Docker container logs to see PIDs and model loading/caching messages from Gunicorn workers.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
