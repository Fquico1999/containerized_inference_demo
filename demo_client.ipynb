{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API Base URL: http://localhost:8080/api\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import time\n",
    "import asyncio\n",
    "import aiohttp\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import nest_asyncio\n",
    "import json\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "API_BASE_URL = \"http://localhost:8080/api\"\n",
    "\n",
    "print(f\"API Base URL: {API_BASE_URL}\\n\")\n",
    "\n",
    "# --- Helper function to make requests ---\n",
    "def make_request(method, endpoint, payload=None, expected_status=200):\n",
    "    url = f\"{API_BASE_URL}{endpoint}\"\n",
    "    try:\n",
    "        if method.upper() == \"GET\":\n",
    "            response = requests.get(url, timeout=60)\n",
    "        elif method.upper() == \"POST\":\n",
    "            response = requests.post(url, json=payload, timeout=120) # Longer timeout for predict\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported method: {method}\")\n",
    "        \n",
    "        print(f\"--- Request to {method.upper()} {endpoint} ---\")\n",
    "        if payload:\n",
    "            print(f\"Payload (first 50 chars if long): {str(payload)[:150]}\")\n",
    "        \n",
    "        if response.status_code == expected_status:\n",
    "            print(f\"Status: {response.status_code} OK\")\n",
    "            try:\n",
    "                res_json = response.json()\n",
    "                print(f\"Response (sample): {str(res_json)[:300]}...\")\n",
    "                return res_json\n",
    "            except requests.exceptions.JSONDecodeError:\n",
    "                print(f\"Response (not JSON): {response.text[:300]}...\")\n",
    "                return response.text\n",
    "        else:\n",
    "            print(f\"Status: {response.status_code} - Error: {response.text[:300]}...\")\n",
    "            return {\"error\": response.text, \"status_code\": response.status_code}\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Request failed for {endpoint}: {e}\")\n",
    "        return {\"error\": str(e), \"status_code\": \"N/A\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Checking Basic API Endpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Checking Basic API Endpoints ---\n",
      "--- Request to GET / ---\n",
      "Status: 200 OK\n",
      "Response (sample): {'message': 'HuggingFace Dynamic Inference API', 'active_device': 'GPU', 'default_model_status': 'Preload attempted, cache populated', 'current_cache_size': 2}...\n",
      "--- Request to GET /healthz ---\n",
      "Status: 200 OK\n",
      "Response (sample): {'status': 'ok', 'pid': 26}...\n",
      "--- Request to GET /readiness ---\n",
      "Status: 200 OK\n",
      "Response (sample): {'status': 'ready', 'message': \"Default model 'distilbert-base-uncased-finetuned-sst-2-english' accessible/loadable.\", 'pid': 26}...\n",
      "--- Request to GET /metrics ---\n",
      "Status: 200 OK\n",
      "Response (not JSON): # HELP hf_requests_total Total number of inference requests\n",
      "# TYPE hf_requests_total counter\n",
      "hf_requests_total{http_status=\"200\",model_name=\"gpt2\",task=\"text-generation\",worker_pid=\"25\"} 2.0\n",
      "hf_requests_total{http_status=\"200\",model_name=\"distilbert-base-uncased-finetuned-sst-2-english\",task=\"sentim...\n",
      "--- Request to GET /cache_info ---\n",
      "Status: 200 OK\n",
      "Response (sample): {'pid': 26, 'lru_cache_stats': 'CacheInfo(hits=9, misses=2, maxsize=10, currsize=2)'}...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'pid': 26,\n",
       " 'lru_cache_stats': 'CacheInfo(hits=9, misses=2, maxsize=10, currsize=2)'}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"\\n--- Checking Basic API Endpoints ---\")\n",
    "make_request(\"GET\", \"/\")\n",
    "make_request(\"GET\", \"/healthz\")\n",
    "make_request(\"GET\", \"/readiness\") # This might fail if default model preload fails\n",
    "make_request(\"GET\", \"/metrics\") # Check Prometheus metrics endpoint\n",
    "make_request(\"GET\", \"/cache_info\") # Check cache on one worker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Single Sentiment Prediction Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Test Single Prediction (Default Model) ---\n",
      "--- Request to POST /predict ---\n",
      "Payload (first 50 chars if long): {'model_name': 'distilbert-base-uncased-finetuned-sst-2-english', 'task': 'sentiment-analysis', 'inputs': 'This is a fantastic product, I highly recom\n",
      "Status: 200 OK\n",
      "Response (sample): {'model_name': 'distilbert-base-uncased-finetuned-sst-2-english', 'task': 'sentiment-analysis', 'predictions': [{'label': 'POSITIVE', 'score': 0.9998854398727417}], 'worker_pid': 27, 'total_request_time_ms': 7.02, 'inference_execution_time_ms': 6.88, 'pipeline_from_lru_cache': True}...\n",
      "\n",
      "Response:\n",
      "model_name: distilbert-base-uncased-finetuned-sst-2-english\n",
      "task: sentiment-analysis\n",
      "predictions: [{'label': 'POSITIVE', 'score': 0.9998854398727417}]\n",
      "worker_pid: 27\n",
      "total_request_time_ms: 7.02\n",
      "inference_execution_time_ms: 6.88\n",
      "pipeline_from_lru_cache: True\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- Test Single Prediction (Default Model) ---\")\n",
    "single_payload_sentiment = {\n",
    "    \"model_name\": \"distilbert-base-uncased-finetuned-sst-2-english\",\n",
    "    \"task\": \"sentiment-analysis\",\n",
    "    \"inputs\": \"This is a fantastic product, I highly recommend it!\"\n",
    "}\n",
    "result_single = make_request(\"POST\", \"/predict\", single_payload_sentiment)\n",
    "print(\"\\nResponse:\")\n",
    "for key, val in result_single.items():\n",
    "    print(f\"{key}: {val}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Batch Sentiment Prediction Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Test Batch Prediction (Default Model) ---\n",
      "--- Request to POST /predict ---\n",
      "Payload (first 50 chars if long): {'model_name': 'distilbert-base-uncased-finetuned-sst-2-english', 'task': 'sentiment-analysis', 'inputs': ['I am incredibly happy with the service.', \n",
      "Status: 200 OK\n",
      "Response (sample): {'model_name': 'distilbert-base-uncased-finetuned-sst-2-english', 'task': 'sentiment-analysis', 'predictions': [{'label': 'POSITIVE', 'score': 0.9998799562454224}, {'label': 'NEGATIVE', 'score': 0.9997836947441101}, {'label': 'POSITIVE', 'score': 0.9969683289527893}], 'worker_pid': 27, 'total_reques...\n",
      "\n",
      "Response:\n",
      "model_name: distilbert-base-uncased-finetuned-sst-2-english\n",
      "task: sentiment-analysis\n",
      "predictions: [{'label': 'POSITIVE', 'score': 0.9998799562454224}, {'label': 'NEGATIVE', 'score': 0.9997836947441101}, {'label': 'POSITIVE', 'score': 0.9969683289527893}]\n",
      "worker_pid: 27\n",
      "total_request_time_ms: 20.76\n",
      "inference_execution_time_ms: 20.63\n",
      "pipeline_from_lru_cache: True\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- Test Batch Prediction (Default Model) ---\")\n",
    "batch_payload_sentiment = {\n",
    "    \"model_name\": \"distilbert-base-uncased-finetuned-sst-2-english\",\n",
    "    \"task\": \"sentiment-analysis\",\n",
    "    \"inputs\": [\n",
    "        \"I am incredibly happy with the service.\",\n",
    "        \"This is the worst thing I have ever bought.\",\n",
    "        \"It's an okay movie, neither good nor bad.\"\n",
    "    ]\n",
    "}\n",
    "result_batch = make_request(\"POST\", \"/predict\", batch_payload_sentiment)\n",
    "print(\"\\nResponse:\")\n",
    "for key, val in result_batch.items():\n",
    "    print(f\"{key}: {val}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Test a Different Model and Task (Text Generation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Test Different Model/Task (Text Generation with GPT-2) ---\n",
      "--- Request to POST /predict ---\n",
      "Payload (first 50 chars if long): {'model_name': 'gpt2', 'task': 'text-generation', 'inputs': 'Once upon a time, in a land far away', 'pipeline_kwargs': {'max_new_tokens': 20, 'num_ret\n",
      "Status: 200 OK\n",
      "Response (sample): {'model_name': 'gpt2', 'task': 'text-generation', 'predictions': [{'generated_text': 'Once upon a time, in a land far away, a man, the son of a rich merchant of a wealthy family, was caught in the midst'}], 'worker_pid': 28, 'total_request_time_ms': 153.7, 'inference_execution_time_ms': 153.56, 'pip...\n",
      "\n",
      "Response:\n",
      "model_name: gpt2\n",
      "task: text-generation\n",
      "predictions: [{'generated_text': 'Once upon a time, in a land far away, a man, the son of a rich merchant of a wealthy family, was caught in the midst'}]\n",
      "worker_pid: 28\n",
      "total_request_time_ms: 153.7\n",
      "inference_execution_time_ms: 153.56\n",
      "pipeline_from_lru_cache: True\n"
     ]
    }
   ],
   "source": [
    "# This will cause a new model to be downloaded and cached if not used before.\n",
    "print(\"\\n--- Test Different Model/Task (Text Generation with GPT-2) ---\")\n",
    "# For actual generation, you might want 'gpt2-medium' or larger.\n",
    "generation_payload = {\n",
    "    \"model_name\": \"gpt2\",\n",
    "    \"task\": \"text-generation\",\n",
    "    \"inputs\": \"Once upon a time, in a land far away\",\n",
    "    \"pipeline_kwargs\": {\"max_new_tokens\": 20, \"num_return_sequences\": 1} # Arguments for the pipeline\n",
    "}\n",
    "# Text generation can take longer, especially for the first load.\n",
    "result_generation = make_request(\"POST\", \"/predict\", generation_payload)\n",
    "\n",
    "print(\"\\nResponse:\")\n",
    "for key, val in result_generation.items():\n",
    "    print(f\"{key}: {val}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parallel Requests using `ThreadPoolExecutor`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Demonstrate Parallel Requests (ThreadPoolExecutor) ---\n",
      "Parallel Req 1/16:\tPID: 28 | Predictions:'[{'label': 'POSITIVE', 'score': 0.9998494386672974}]' | Total Request Time (ms): '5.82'\n",
      "Parallel Req 2/16:\tPID: 25 | Predictions:'[{'label': 'NEGATIVE', 'score': 0.9992559552192688}]' | Total Request Time (ms): '5.02'\n",
      "Parallel Req 3/16:\tPID: 27 | Predictions:'[{'label': 'POSITIVE', 'score': 0.9997918009757996}]' | Total Request Time (ms): '4.52'\n",
      "Parallel Req 4/16:\tPID: 25 | Predictions:'[{'label': 'POSITIVE', 'score': 0.9998791217803955}]' | Total Request Time (ms): '5.17'\n",
      "Parallel Req 5/16:\tPID: 26 | Predictions:'[{'label': 'POSITIVE', 'score': 0.9998459815979004}]' | Total Request Time (ms): '8.09'\n",
      "Parallel Req 6/16:\tPID: 27 | Predictions:'[{'label': 'NEGATIVE', 'score': 0.9865264296531677}]' | Total Request Time (ms): '4.52'\n",
      "Parallel Req 7/16:\tPID: 27 | Predictions:'[{'label': 'NEGATIVE', 'score': 0.9997283816337585}]' | Total Request Time (ms): '7.09'\n",
      "Parallel Req 8/16:\tPID: 25 | Predictions:'[{'label': 'POSITIVE', 'score': 0.9998494386672974}]' | Total Request Time (ms): '5.8'\n",
      "Parallel Req 9/16:\tPID: 28 | Predictions:'[{'label': 'NEGATIVE', 'score': 0.9865264296531677}]' | Total Request Time (ms): '8.15'\n",
      "Parallel Req 10/16:\tPID: 26 | Predictions:'[{'label': 'POSITIVE', 'score': 0.9998791217803955}]' | Total Request Time (ms): '8.78'\n",
      "Parallel Req 11/16:\tPID: 27 | Predictions:'[{'label': 'NEGATIVE', 'score': 0.9992559552192688}]' | Total Request Time (ms): '7.55'\n",
      "Parallel Req 12/16:\tPID: 25 | Predictions:'[{'label': 'NEGATIVE', 'score': 0.9997954964637756}]' | Total Request Time (ms): '6.48'\n",
      "Parallel Req 13/16:\tPID: 27 | Predictions:'[{'label': 'POSITIVE', 'score': 0.9997918009757996}]' | Total Request Time (ms): '3.94'\n",
      "Parallel Req 14/16:\tPID: 28 | Predictions:'[{'label': 'NEGATIVE', 'score': 0.9997954964637756}]' | Total Request Time (ms): '9.01'\n",
      "Parallel Req 15/16:\tPID: 26 | Predictions:'[{'label': 'POSITIVE', 'score': 0.9998459815979004}]' | Total Request Time (ms): '4.8'\n",
      "Parallel Req 16/16:\tPID: 28 | Predictions:'[{'label': 'NEGATIVE', 'score': 0.9997283816337585}]' | Total Request Time (ms): '4.78'\n",
      "\n",
      "ThreadPoolExecutor: Completed 16 sentiment requests in 0.06 seconds.\n",
      "Sentiment requests handled by PIDs: {25, 26, 27, 28} (16 successful)\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- Demonstrate Parallel Requests (ThreadPoolExecutor) ---\")\n",
    "parallel_texts_sentiment = [\n",
    "    \"The weather today is beautiful and sunny.\",\n",
    "    \"I'm feeling a bit down after hearing the news.\",\n",
    "    \"This new software update is incredibly buggy.\",\n",
    "    \"The concert was an unforgettable experience!\",\n",
    "    \"Customer support was surprisingly helpful and efficient.\",\n",
    "    \"I am neutral about this new policy change.\",\n",
    "    \"This book is a masterpiece of modern literature.\",\n",
    "    \"The food at that restaurant was utterly disappointing.\"\n",
    "] * 2 # 16 requests\n",
    "\n",
    "def send_predict_request(text_input, model_name, task):\n",
    "    payload = {\"model_name\": model_name, \"task\": task, \"inputs\": text_input}\n",
    "    url = f\"{API_BASE_URL}/predict\"\n",
    "    try:\n",
    "        response = requests.post(url, json=payload, timeout=120)\n",
    "        response.raise_for_status()\n",
    "        return response.json()\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        return {\"error\": str(e), \"input_text\": text_input, \"status_code\": response.status_code if 'response' in locals() else \"N/A\"}\n",
    "\n",
    "start_time_parallel = time.time()\n",
    "parallel_results_sentiment = []\n",
    "# Aggressively use workers to demonstrate server parallelism\n",
    "with ThreadPoolExecutor(max_workers=len(parallel_texts_sentiment)) as executor:\n",
    "    futures = [\n",
    "        executor.submit(send_predict_request, text, \"distilbert-base-uncased-finetuned-sst-2-english\", \"sentiment-analysis\")\n",
    "        for text in parallel_texts_sentiment\n",
    "    ]\n",
    "    for i, future in enumerate(as_completed(futures)):\n",
    "        try:\n",
    "            data = future.result()\n",
    "            parallel_results_sentiment.append(data)\n",
    "            print(f\"Parallel Req {i+1}/{len(parallel_texts_sentiment)}:\\tPID: {data.get('worker_pid', 'N/A')} | Predictions:'{str(data.get('predictions', 'N/A'))}' | Total Request Time (ms): '{str(data.get('total_request_time_ms', 'N/A'))}'\")\n",
    "        except Exception as exc:\n",
    "            print(f\"Parallel Req {i+1}/{len(parallel_texts_sentiment)} generated an exception: {exc}\")\n",
    "            parallel_results_sentiment.append({\"error\": str(exc)})\n",
    "\n",
    "end_time_parallel = time.time()\n",
    "print(f\"\\nThreadPoolExecutor: Completed {len(parallel_results_sentiment)} sentiment requests in {end_time_parallel - start_time_parallel:.2f} seconds.\")\n",
    "\n",
    "worker_pids_sentiment = set()\n",
    "successful_sentiment_requests = 0\n",
    "for res in parallel_results_sentiment:\n",
    "    if isinstance(res, dict) and \"worker_pid\" in res:\n",
    "        worker_pids_sentiment.add(res['worker_pid'])\n",
    "        successful_sentiment_requests +=1\n",
    "print(f\"Sentiment requests handled by PIDs: {worker_pids_sentiment} ({successful_sentiment_requests} successful)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
